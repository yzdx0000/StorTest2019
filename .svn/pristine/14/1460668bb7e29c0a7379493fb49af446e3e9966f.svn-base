# -*- coding:utf-8 _*-
"""
测试内容:一一节点一块元数据盘故障，超时后恢复

步骤:
1、在节点A创建LUN1-LUN6
2、将逻辑卷映射至主机1上LUN1-LUN6
3、在主机1上运行vdbench -f min-seq-w-1l.conf -jn；
4、业务完成拔掉一块元数据盘；
5、等待超时后，触发被动重建；
6、数据重建完成后，比较内部数据一致性
7、插回步骤4拔出的数据盘。
检查项:
1、步骤4、拔掉一块数据盘，业务不受影响，系统上报磁盘异常信息；
2、步骤5、持续修改写数据，立即触发数据修复，超时内恢复数据盘，系统校验数据；
3、步骤6、修复过程中，内外部数据校验一致；
4,、步骤7、修复完成后，内部数据校验一致。

"""

import os, sys
import utils_path
import common2
import common
import Lun_managerTest
import log
import get_config
import threading
import login
import time
import commands
import random
import breakdown
import ReliableTest
# import error

import env_manage_repair_test
import decorator_func
from get_config import config_parser as cp

"""初始化日志和全局变量"""
file_name = os.path.basename(__file__)
file_name = file_name[:-3]  # 获取本脚本名，去掉.py后缀
log_file_path = log.get_log_path(file_name)  # 获取日志目录，即test_cases/Log/Case_log
log.init(log_file_path, True)  # 初始化日志文件

log.info("---------------全局初始化操作-----------------")
node_ip1 = env_manage_repair_test.deploy_ips[0]
node_ip2 = env_manage_repair_test.deploy_ips[1]
client_ip1 = env_manage_repair_test.client_ips[0]
esxi_ip = env_manage_repair_test.Esxi_ips
fault_ip = random.choice(env_manage_repair_test.deploy_ips)
node_id = env_manage_repair_test.com_node.get_node_id_by_ip(fault_ip)  # 随机选取一个节点

"""获取指定节点数据盘和共享盘"""
share_disk, data_disk = env_manage_repair_test.Reliable_osan.get_share_monopoly_disk_ids(s_ip=fault_ip, node_id=node_id)
disk_ids = []
for share_disk in share_disk:
    disk_phy_id = env_manage_repair_test.Reliable_osan.get_physicalid_by_name(fault_ip, share_disk)
    disk_ids.append(disk_phy_id)
disk_phy_id = random.choice(disk_ids)  # 磁盘物理id
disk_name = env_manage_repair_test.Reliable_osan.get_name_by_physicalid(fault_ip, disk_phy_id)  # 磁盘名字
disk_uuid = env_manage_repair_test.com_disk.get_disk_uuid_by_name(node_id=node_id, disk_name=disk_name)  # 磁盘uuid
disk_id = env_manage_repair_test.Reliable_osan.get_diskid_by_name(s_ip=fault_ip, node_id=node_id,
                                                                  disk_name=disk_name)  # 集群中的磁盘id
stor_id_block = env_manage_repair_test.Lun_osan.get_storage__type_id(s_ip=fault_ip, type="SHARED")


def iscsi_login():
    global min_seq_w
    global min_seq_r
    login.login()

    # 修改vdbench配置文件的参数值
    #seekpct = 0  # 随机
    rdpct1 = 0  # 读写比例(0是全写)
    rdpct2 = 100
    xfersize1 = cp("vdbench", "unmix_xfersize1")
    lun1 = env_manage_repair_test.Lun_osan.ls_scsi_dev(client_ip1)
    min_seq_w = env_manage_repair_test.co2_osan.gen_vdb_xml(max_range=cp("vdbench", "range"),
                                                            maxdata=cp("vdbench", "maxdata"),
                                                            thread=cp("vdbench", "threads"), lun=lun1,
                                                            xfersize=xfersize1, seekpct=cp("vdbench", "seekpct"), rdpct=rdpct1)
    min_seq_r = env_manage_repair_test.co2_osan.gen_vdb_xml(max_range=cp("vdbench", "range"),
                                                            maxdata=cp("vdbench", "maxdata"),
                                                            thread=cp("vdbench", "threads"), lun=lun1,
                                                            xfersize=xfersize1, seekpct=cp("vdbench", "seekpct"), rdpct=rdpct2)


def run_vdb_w(arg=0):
    log.info('Run task %s (%s)...' % (sys._getframe().f_code.co_name, os.getpid()))
    env_manage_repair_test.co2_osan.run_vdb(client_ip=client_ip1, vdb_xml=min_seq_w, jn_jro='jn', output=fault_ip)


def disk_error(arg=0):
    log.info('Run task %s (%s)...' % (sys._getframe().f_code.co_name, os.getpid()))
    decorator_func.timer(int(cp("wait_time", "remove_disk")))  # 执行vdbench期间，先等待时间再开始磁盘故障的操作
    log.info(disk_ids)
    env_manage_repair_test.Reliable_osan.remove_disk(node_ip=fault_ip, disk_id=disk_phy_id, disk_usage="SHARED")


def vdb_jn(confile, type):
    log.info("Run vdbench with jn.")
    env_manage_repair_test.co2_osan.run_vdb(client_ip1, confile, output=fault_ip, jn_jro=type)


def case():
    log.info("生成vdbench配置文件，主机端登录 ...")
    iscsi_login()
    log.info("在主机1对lun1上运行vdbench -f min-seq-w-1l.conf -jn ...")
    env_manage_repair_test.co2_osan.run_vdb(client_ip=client_ip1, vdb_xml=min_seq_w, jn_jro='jn', output=fault_ip)
    log.info("业务完成拔掉一块元数据盘")
    disk_error()
    log.info("等待超时后，触发被动重建...")
    decorator_func.timer(10)
    env_manage_repair_test.Reliable_osan.run_down_disk_wait(s_ip=fault_ip,
                                                            timeout=int(cp('timeout', 'disk_timeout')))  # 设置修复的超时时间

    decorator_func.timer(int(cp('timeout', 'disk_timeout')) /int(cp('timeout', 'unit_time'))+ 10)  # 设置等待超时的时间
    log.info("数据重建完成后，比较内部数据一致性 ...")
    env_manage_repair_test.Reliable_osan.check_disk_state(node_ip=fault_ip,
                                                          disk_uuid=disk_uuid,
                                                          disk_state="DISK_STATE_ZOMBIE")  # 检测磁盘状态，若为ZOMBIE，认为重建完成
    env_manage_repair_test.Reliable_osan.compare_data()

    env_manage_repair_test.Reliable_osan.insert_disk(node_ip=fault_ip, disk_id=disk_phy_id, disk_usage="SHARED")
    time.sleep(10)
    env_manage_repair_test.com_disk.remove_disks(disk_ids=disk_id)
    env_manage_repair_test.Reliable_osan.add_disks(s_ip=fault_ip, node_ids=node_id, uuid=disk_uuid, usage="SHARED",
                                                   storage_id=stor_id_block)


def main():
    env_manage_repair_test.rel_check_before_run(filename=file_name)  # 环境检测和准备

    case()  # 用例步骤

    common.ckeck_system()  # 检查系统core

    env_manage_repair_test.clean()  # 环境清理


if __name__ == '__main__':
    common.case_main(main)  # 主函数执行入口
