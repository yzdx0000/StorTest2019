# -*- coding:utf-8 _*-
"""
测试内容:数据修改写时，同磁盘组多块元数据盘故障

步骤:
1、在节点A创建lun1-lun6
2、将逻辑卷映射至主机1上lun1-lun6
3、在主机1上运行vdbench -f min-seq-write.conf -jn；
4、在步骤3业务进行过程中，将同磁盘组多块元数据盘故障；
5、故障盘超时后，立即开始数据修复
6、数据修复过程中，主机端执行vdbench -f min-seq-write.conf -jro校验数据一致性
7、数据修复完成后，比较内部数据一致性

检查项:
1、步骤4、拔掉多块元数据盘，业务不受影响，系统上报磁盘异常信息；
2、步骤5、持续修改写数据，故障盘超时后触发数据修复；
3、步骤6、修复过程中，内外部数据校验一致；
4,、步骤7、修复完成后，内部数据校验一致。

"""

# testlink case: 1000-33208
import os, sys
import utils_path
import common2
import common
import Lun_managerTest
import log
import get_config
import threading
import login
import time
import commands
import random
import breakdown
import ReliableTest
# import error

import env_manage_repair_test
import decorator_func
from get_config import config_parser as cp

"""初始化日志和变量"""
FILE_NAME = os.path.splitext(os.path.basename(__file__))[0]
file_name = os.path.basename(__file__)
file_name = file_name[:-3]  # 获取本脚本名，去掉.py后缀
log_file_path = log.get_log_path(file_name)  # 获取日志目录，即test_cases/Log/Case_log
log.init(log_file_path, True)  # 初始化日志文件

node_ip1 = env_manage_repair_test.deploy_ips[0]
node_ip2 = env_manage_repair_test.deploy_ips[1]
client_ip1 = env_manage_repair_test.client_ips[0]
esxi_ip = env_manage_repair_test.Esxi_ips
node_id = env_manage_repair_test.com_node.get_node_id_by_ip(node_ip1)  # 随机选取一个节点
print node_id

"""获取指定节点数据盘和共享盘"""
share_disk, data_disk = env_manage_repair_test.Reliable_osan.get_share_monopoly_disk_ids(s_ip=node_ip1, node_id=node_id)
disk_ids = []
for share_disk in share_disk:
    disk_phy_id = env_manage_repair_test.Reliable_osan.get_physicalid_by_name(node_ip1, share_disk)
    disk_ids.append(disk_phy_id)
disk_phy_id = disk_ids[0]  # 磁盘物理id
disk_name = env_manage_repair_test.Reliable_osan.get_name_by_physicalid(node_ip1, disk_phy_id)  # 磁盘名字
disk_uuid = env_manage_repair_test.com_disk.get_disk_uuid_by_name(node_id=node_id, disk_name=disk_name)  # 磁盘uuid
disk_id = env_manage_repair_test.Reliable_osan.get_diskid_by_name(s_ip=node_ip1, node_id=node_id,
                                                                  disk_name=disk_name)  # 集群中的磁盘id

disk_phy_id2 = disk_ids[1]
disk_name2 = env_manage_repair_test.Reliable_osan.get_name_by_physicalid(node_ip1, disk_phy_id2)  # 磁盘名字
disk_uuid2 = env_manage_repair_test.com_disk.get_disk_uuid_by_name(node_id=node_id, disk_name=disk_name2)  # 磁盘uuid
disk_id2 = env_manage_repair_test.Reliable_osan.get_diskid_by_name(s_ip=node_ip1, node_id=node_id,
                                                                   disk_name=disk_name2)  # 集群中的磁盘id
stor_id_block = env_manage_repair_test.Lun_osan.get_storage__type_id(s_ip=node_ip1, type="SHARED")

"""计算集群中所有lun大小之和,因为主机只映射了一半的lun，根据此值设置vdbench  maxdata的值是实际lun大小的2倍"""
lun_ids = env_manage_repair_test.Lun_osan.get_lun(node_ip1)
total_lun_size = 0.0
for lun_id in lun_ids:
    total_lun_size = total_lun_size + (env_manage_repair_test.Lun_osan.get_option_single(s_ip=node_ip1,
                                                                                         command='get_luns', ids="ids",
                                                                                         indexname="luns",
                                                                                         argv2="total_bytes",
                                                                                         argv1=lun_id)) / 1024 / 1024
print total_lun_size
vdbench_lun_size = "{}G".format(round(total_lun_size / 1024, 2))
log.info("将vdbench的maxdata值设置为集群主机映射lun总大小的2倍，为{}".format(vdbench_lun_size))


def iscsi_login():
    global lun1_min_seq_w
    global lun2_min_seq_w
    global min_seq_w
    global min_seq_r
    login.login()

    # 修改vdbench配置文件的参数值
    #seekpct = 0  # 随机
    rdpct1 = 0  # 读写比例(0是全写)
    rdpct2 = 100
    xfersize1 = cp("vdbench", "unmix_xfersize1")
    lun1 = env_manage_repair_test.Lun_osan.ls_scsi_dev(client_ip1)
    min_seq_w = env_manage_repair_test.co2_osan.gen_vdb_xml(lun=lun1, xfersize=xfersize1, seekpct=cp("vdbench", "seekpct"),
                                                            rdpct=rdpct1, maxdata=vdbench_lun_size)
    min_seq_r = env_manage_repair_test.co2_osan.gen_vdb_xml(lun=lun1, xfersize=xfersize1, seekpct=cp("vdbench", "seekpct"),
                                                            rdpct=rdpct2, maxdata=vdbench_lun_size)


def run_vdb_w(arg=0):
    log.info('Run task %s (%s)...' % (sys._getframe().f_code.co_name, os.getpid()))
    env_manage_repair_test.co2_osan.run_vdb(client_ip=client_ip1, vdb_xml=min_seq_w, jn_jro='jn', output=node_ip1)


def run_vdb_lun1_w(arg=0):
    log.info('Run task %s (%s)...' % (sys._getframe().f_code.co_name, os.getpid()))
    env_manage_repair_test.co2_osan.run_vdb(client_ip=client_ip1, vdb_xml=lun1_min_seq_w, jn_jro='jn', output=node_ip1)


def run_vdb_lun2_w(arg=0):
    log.info('Run task %s (%s)...' % (sys._getframe().f_code.co_name, os.getpid()))
    env_manage_repair_test.co2_osan.run_vdb(client_ip=client_ip1, vdb_xml=lun2_min_seq_w, jn_jro='jn', output=node_ip1)


def disk_error(arg=0):
    log.info('Run task %s (%s)...' % (sys._getframe().f_code.co_name, os.getpid()))
    time.sleep(int(cp("wait_time", "remove_disk")))  # 执行vdbench期间，先等待时间再开始磁盘故障的操作
    log.info(disk_ids)
    env_manage_repair_test.Reliable_osan.remove_disk(node_ip=node_ip1, disk_id=disk_ids[0], disk_usage="SHARED")
    env_manage_repair_test.Reliable_osan.remove_disk(node_ip=node_ip1, disk_id=disk_ids[1], disk_usage="SHARED")


def vdb_jn(confile, type):
    log.info("Run vdbench with jn.")
    env_manage_repair_test.co2_osan.run_vdb(client_ip1, confile, output=node_ip1, jn_jro=type)


def case():
    log.info("生成vdbench配置文件，主机端登录 ...")
    iscsi_login()
    log.info("在主机1对lun1上运行vdbench -f min-seq-w-1l.conf -jn ...")
    env_manage_repair_test.co2_osan.run_vdb(client_ip=client_ip1, vdb_xml=min_seq_w, jn_jro='jn', output=node_ip1)
    log.info("在步骤3业务进行过程中，将lun写满后，再重新执行步骤3业务中将一个数据节点上,将同磁盘组多块元数据盘故障")
    decorator_func.multi_threads(run_vdb_w, disk_error)
    log.info("故障盘超时后，立即开始数据修复...")
    env_manage_repair_test.Reliable_osan.run_down_disk_wait(node_ip1, int(cp("timeout", "disk_timeout")))
    time.sleep(int(cp("timeout", "disk_timeout"))/int(cp('timeout', 'unit_time')) + 30)

    log.info(
        "数据修复过程中，主机端执行vdbench -f min-seq-write.conf -jro校验数据一致性")
    env_manage_repair_test.co2_osan.run_vdb(client_ip=client_ip1, vdb_xml=min_seq_r, jn_jro='jro', output=node_ip1)
    log.info("数据修复完成后，比较内部数据一致性 ...")
    env_manage_repair_test.Reliable_osan.insert_disk(node_ip=node_ip1, disk_id=disk_ids[0], disk_usage="SHARED")
    env_manage_repair_test.Reliable_osan.insert_disk(node_ip=node_ip1, disk_id=disk_ids[1], disk_usage="SHARED")
    env_manage_repair_test.Reliable_osan.check_disk_state(node_ip=node_ip1,
                                                          disk_uuid=disk_uuid,
                                                          disk_state="DISK_STATE_ZOMBIE")  # 检测磁盘状态，若为ZOMBIE，认为重建完成
    env_manage_repair_test.Reliable_osan.check_disk_state(node_ip=node_ip1,
                                                          disk_uuid=disk_uuid2,
                                                          disk_state="DISK_STATE_ZOMBIE")  # 检测磁盘状态，若为ZOMBIE，认为重建完成
    env_manage_repair_test.com_disk.remove_disks(disk_ids=disk_id)
    env_manage_repair_test.com_disk.remove_disks(disk_ids=disk_id2)

    env_manage_repair_test.Reliable_osan.add_disks(s_ip=node_ip1, node_ids=node_id, uuid=disk_uuid, usage="SHARED",
                                                   storage_id=stor_id_block)
    env_manage_repair_test.Reliable_osan.add_disks(s_ip=node_ip1, node_ids=node_id, uuid=disk_uuid2, usage="SHARED",
                                                   storage_id=stor_id_block)
    env_manage_repair_test.co2_osan.run_vdb(client_ip=client_ip1, vdb_xml=min_seq_w, output=node_ip1)

    env_manage_repair_test.Reliable_osan.compare_data()


def main():
    env_manage_repair_test.rel_check_before_run(filename=file_name)  # 环境检测和准备

    case()  # 用例步骤

    common.ckeck_system()  # 检查系统core

    env_manage_repair_test.clean()  # 环境清理


if __name__ == '__main__':
    common.case_main(main)  # 主函数执行入口
